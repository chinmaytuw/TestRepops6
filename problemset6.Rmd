---
title: 'INFX 573: Problem Set 6 - Regression'
author: "Chinmay Tatwawadi"
date: 'Due: Tuesday, November 15, 2016'
output:
  html_document: default
  pdf_document: default
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: 

##### Instructions: #####

Before beginning this assignment, please ensure you have access to R and RStudio. 

1. Download the `problemset6.Rmd` file from Canvas. Open `problemset6.Rmd` in RStudio and supply your solutions to the assignment by editing `problemset6.Rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name. Any collaborators must be listed on the top of your assignment. 

3. Be sure to include well-documented (e.g. commented) code chucks, figures and clearly written text chunk explanations as necessary. Any figures should be clearly labeled and appropriately referenced within the text. 

4. Collaboration on problem sets is acceptable, and even encouraged, but each student must turn in an individual write-up in his or her own words and his or her own work. The names of all collaborators must be listed on each assignment. Do not copy-and-paste from other students' responses or code.

5. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the R Markdown file to `YourLastName_YourFirstName_ps6.Rmd`, knit a PDF and submit the PDF file on Canvas.

##### Setup: #####

In this problem set you will need, at minimum, the following R packages.

```{r Setup, message=FALSE}
# Load standard libraries
library(tidyverse)
library(dplyr) # added dplyr
library(car)
library(MASS) # Modern applied statistics functions
```

\textbf{Housing Values in Suburbs of Boston}

In this problem we will use the Boston dataset that is available in the \texttt{MASS} package. This dataset contains information about median house value for 506 neighborhoods in Boston, MA. Load this data and use it to answer the following questions.

\benum

\item Describe the data and variables that are part of the \texttt{Boston} dataset. Tidy data as necessary.
```{r}
#describe(Boston)
```

- This is a data frame with 14 Variables and 506 observations.
- It describes the hosuing values of the different suburbs of Boston
- The different variables are:
1. crim 	per capita crime rate by town.
2. zn 	proportion of residential land zoned for lots over 25,000 sq.ft.
3. indus 	proportion of non-retail business acres per town.
4. chas 	Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
5. nox 	nitrogen oxides concentration (parts per 10 million).
6. rm	 average number of rooms per dwelling.
7. age 	proportion of owner-occupied units built prior to 1940.
8. dis 	weighted mean of distances to five Boston employment centres.
9. rad 	index of accessibility to radial highways.
10. tax 	full-value property-tax rate per \$10,000.
11. ptratio pupil-teacher ratio by town.
12. black 	1000(Bk ??? 0.63)2 where Bk is the proportion of blacks by town.
13. lstat 	lower status of the population (percent).
14. medv 	median value of owner-occupied homes in \$1000s.



\item Consider this data in context, what is the response variable of interest? Discuss how you think some of the possible predictor variables might be associated with this response.

Answer:
- medv i.e. the median value of homes is the response variable.

- Many factors could affect the median value. Some of which could be:

1. crim 	per capita crime rate by town- less the crime, higher the value
2. nox 	nitrogen oxides concentration, less the conc, better the area
3. rm	 average number of rooms per dwelling, more the rooms, higher the value
4. dis 	weighted mean of distances to five Boston employment centres- closer the better
5. lstat 	lower status of the population- higher the status, better the value
6. black  proportion of blacks by town - it will be interesting to see if there is a racial angle to this. 
7. ptratio pupil-teacher ratio by town- lower the ratio, higher should be the value of the area.

```{r}
cor(Boston$medv,Boston)
```

So we can see that rm has the highest positive correlation. Whereas, lstat and ptratio have the highest negative correlation.

\item For each predictor, fit a simple linear regression model to predict the response. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions. 
```{r}
# 1. medv and crim
lm.crim <- lm(data=Boston, medv~crim)
summary(lm.crim)

# 2. medv and zn
lm.zn <- lm(data=Boston, medv~zn)
summary(lm.zn)

# 3. medv and indus
lm.indus <- lm(data=Boston, medv~indus)
summary(lm.indus)

# 4. medv and chas
lm.chas <- lm(data=Boston, medv~chas)
summary(lm.chas)

# 5. medv and nox
lm.nox <- lm(data=Boston, medv~nox)
summary(lm.nox)

# 6. medv and rm
lm.rm <- lm(data=Boston, medv~rm)
summary(lm.rm)

# 7. medv and age
lm.age <- lm(data=Boston, medv~age)
summary(lm.age)

# 8. medv and dis
lm.dis <- lm(data=Boston, medv~dis)
summary(lm.dis)

# 9. medv and rad
lm.rad <- lm(data=Boston, medv~rad)
summary(lm.rad)

# 10. medv and tax
lm.tax <- lm(data=Boston, medv~tax)
summary(lm.tax)

# 11. medv and ptratio
lm.ptratio <- lm(data=Boston, medv~ptratio)
summary(lm.ptratio)

# 12. medv and black
lm.black <- lm(data=Boston, medv~black)
summary(lm.black)

# 13. medv and lstat
lm.lstat <- lm(data=Boston, medv~lstat)
summary(lm.lstat)
```



Analysis:
So we can see that rm and lstat have the highest significance:
1. Their p value are the almost 0.
2. Their r squared values are among the highest.

```{r}
#plot 1: lstat Vs Medv
ggplot(data=Boston, aes(x=lstat, y=medv)) +
  geom_point(col="red",alpha=0.6 ) +
    geom_smooth()+
  labs(title="Regression for lstat Vs medv", x="lstat", y="medv")


#plot 2: rm Vs Medv
ggplot(data=Boston, aes(x=rm, y=medv)) +
  geom_point(col="red",alpha=0.6 ) +
    geom_smooth()+
  labs(title="Regression for rm Vs medv", x="rm", y="medv")

```

\item Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0: \beta_j = 0$?
```{r}
lm.allPred <- lm(data=Boston, medv~.)
summary(lm.allPred)
```

Analysis:
1. We can see that p value for F-stat is significant. So there are definitely relationships
between the variables.
2. Based on the p values, we can reject null hypothesis for:
- crim, indus,chas,age,tax.


\item How do your results from (3) compare to your results from (4)? Create a plot displaying the univariate regression coefficients from (3) on the x-axis and the multiple regression coefficients from part (4) on the y-axis. Use this visualization to support your response.
```{r}
#Creating Data frame for multiple regression coeff:
df_allPred <- data.frame(lm.allPred$coefficients)

#Clean the data:
df_allPred <- data.frame(df_allPred[2:14,])

#Naming the cols
names(df_allPred) <- c("MultipleCoeff")

#Creating Data frame for simple regression coeff:
df_uniPred <- data.frame(c(lm.crim$coefficients[2], lm.zn$coefficients[2],
                          lm.indus$coefficients[2],lm.chas$coefficients[2],
                          lm.nox$coefficients[2],lm.rm$coefficients[2],lm.age$coefficients[2],
                          lm.dis$coefficients[2],lm.rad$coefficients[2],lm.tax$coefficients[2],
                          lm.ptratio$coefficients[2],lm.black$coefficients[2],
                          lm.lstat$coefficients[2] ))


#Naming the cols
names(df_uniPred) <- c("UniCoeff")


#Creating combined table:
ComTB <- data.frame(c(df_uniPred, df_allPred))


#displaying the table
ComTB

#plotting linear regression coefs on x axis & multiple regression coefs on y axis
ggplot(ComTB, aes(x = UniCoeff, y = MultipleCoeff)) +
  geom_point()


```
Analysis:
1. rm and chas are the 2 significant outliers. We already knew that rm was significant but
the impact of chas is a new revelation.
2. lstat has little impact on response despite high correlation


\item Is there evidence of a non-linear association between any of the predictors and the response? To answer this question, for each predictor $X$ fit a model of the form:

$$ Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon $$
```{r}
#fitting a 3rd degree exponential model for each predictor variable
poly_crim <- lm(medv~poly(crim,3), data = Boston)
poly_zn <- lm(medv~poly(zn,3), data = Boston)
poly_indus <- lm(medv~poly(indus,3), data = Boston)
poly_nox <- lm(medv~poly(nox,3), data = Boston)
poly_rm <- lm(medv~poly(rm,3), data = Boston)
poly_age <- lm(medv~poly(age,3), data = Boston)
poly_dis <- lm(medv~poly(dis,3), data = Boston)
poly_rad <- lm(medv~poly(rad,3), data = Boston)
poly_tax <- lm(medv~poly(tax,3), data = Boston)
poly_ptratio <- lm(medv~poly(ptratio,3), data = Boston)
poly_black <- lm(medv~poly(black,3), data = Boston)
poly_lstat <- lm(medv~poly(lstat,3), data = Boston)

#checking summaries of each model to check the p value for the 3rd degree term
summary(poly_crim)
summary(poly_zn)
summary(poly_indus)
summary(poly_nox)
summary(poly_rm)
summary(poly_age)
summary(poly_dis)
summary(poly_rad)
summary(poly_tax)
summary(poly_ptratio)
summary(poly_black)
summary(poly_lstat)
```
Analysis:
Variables age, tax, ptratio, black have insignificant p values so we can conclude that there is no
non-linear correlation. For the other variables, p value is high so there is a non linear correlation.

\item Consider performing a stepwise model selection procedure to determine the bets fit model. Discuss your results. How is this model different from the model in (4)?

```{r}
#stepwise model selection:
lm_stepwiseSel <- stepAIC(lm.allPred ,direction="both")
lm_stepwiseSel$anova
```

Analysis:
In Stepwise selection model, we remove the variables age,indus as this gives us the least AIC value.
Earlier we rejected null hypothesis for: crim, indus,chas,age,tax. age,indus has the highest p values
in the group as well.
This model has 2 less variables. Let us try to plot residuals Vs Fitted for both the models
and see which one is better:

```{r}
ggplot(data = lm_stepwiseSel, aes(x=lm_stepwiseSel$fitted.values, y=lm_stepwiseSel$residuals) )+
  geom_point(alpha=0.4,size=3,col="red") +
  geom_smooth(method="lm",col="blue")+
  labs(title="Fitted Vs Residuals", x="StepWise_Fitted", y="StepWise_Residuals")

ggplot(data=lm.allPred, aes(x=lm.allPred$fitted.values, y=lm.allPred$residuals ) )+
  geom_point(alpha=0.4,size=3,col="red") +
  geom_smooth(method="lm",col="blue")+
  labs(title="Fitted Vs Residuals", x="Multi Regression_fitted", y="Multi Regression Residuals")
```

So, it seems that the 2 models are almost the same. There is hardly any difference in plots here.

\item Evaluate the statistical assumptions in your regression analysis from (7) by performing a basic analysis of model residuals and any unusual observations. Discuss any concerns you have about your model.
\eenum
```{r}
ggplot(data = lm_stepwiseSel, aes(x=lm_stepwiseSel$fitted.values, y=lm_stepwiseSel$residuals) )+
  geom_point(alpha=0.4,size=3,col="red") +
  geom_smooth(method="lm",col="blue")+
  labs(title="Fitted Vs Residuals", x="StepWise_Fitted", y="StepWise_Residuals")
```
Analysis:

1. From the model's Fitted Vs residuals curve, we see that most of the points are concentrated near the mean line. 
2. We can also see that most value between 10 and 30 seem to have negative residual whereas values
less than 10 and greater than 30 tend to have positive residual values. This means that the variation
is not constant throughout the plot and violates the homoscedasticity rule.
3. We cann also see a few outliers on the top right. They seem to form a pattern. This pattern
can be observed on the similar model for simple linear regression as well.
4. The overall model seems to be non linear and has a certain amount of curvature.


Concerns:
1. I have my reservations for this model as it seems to be violating a few rules- nonlinearity, homoscedasticity.
2. Also, the outliers in the top right might suggest that regression model might not be the
best model.
3. A counter can be argued by saying that the model doesn't seem to be overfitting and the
model is robust.


